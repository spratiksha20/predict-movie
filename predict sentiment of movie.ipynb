{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lect2-last.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfmfCFeNwdmb",
        "outputId": "8a09381b-2112-49e9-8e26-47071ab4ff9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir ../data\n",
        "!wget -O ../data/aclImdb_v1.tar.gz https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTRta0lZw9FH",
        "outputId": "b46c40d9-87e5-4768-9a7d-e47bbee142f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-31 20:12:49--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
            "\n",
            "../data/aclImdb_v1. 100%[===================>]  80.23M  23.8MB/s    in 4.4s    \n",
            "\n",
            "2022-01-31 20:12:54 (18.3 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def read_imdb_data(data_dir='../data/aclImdb'):\n",
        "    data = {}\n",
        "    labels = {}\n",
        "    \n",
        "    for data_type in ['train', 'test']:\n",
        "        data[data_type] = {}\n",
        "        labels[data_type] = {}\n",
        "        \n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            data[data_type][sentiment] = []\n",
        "            labels[data_type][sentiment] = []\n",
        "            \n",
        "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
        "            files = glob.glob(path)\n",
        "            \n",
        "            for f in files:\n",
        "                with open(f) as review:\n",
        "                    data[data_type][sentiment].append(review.read())\n",
        "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
        "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
        "                    \n",
        "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
        "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
        "                \n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "knPqaC0txRtm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, labels = read_imdb_data()\n",
        "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
        "            len(data['train']['pos']), len(data['train']['neg']),\n",
        "            len(data['test']['pos']), len(data['test']['neg'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vgLoiIyxpQ7",
        "outputId": "9204ad2f-5528-484d-d3a2-77bd1a0b04ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def prepare_imdb_data(data, labels):\n",
        "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
        "    \n",
        "    #Combine positive and negative reviews and labels\n",
        "    data_train = data['train']['pos'] + data['train']['neg']\n",
        "    data_test = data['test']['pos'] + data['test']['neg']\n",
        "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
        "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
        "    \n",
        "    #Shuffle reviews and corresponding labels within training and test sets\n",
        "    data_train, labels_train = shuffle(data_train, labels_train)\n",
        "    data_test, labels_test = shuffle(data_test, labels_test)\n",
        "    \n",
        "    # Return a unified training data, test data, training labels, test labets\n",
        "    return data_train, data_test, labels_train, labels_test"
      ],
      "metadata": {
        "id": "yIXVNistxtHh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
        "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDI8qoG6xw9p",
        "outputId": "25a06512-2d22-479c-80ad-635201e33ce9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDb reviews (combined): train = 25000, test = 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "Ly7v8pQ1x0ht",
        "outputId": "77f67c43-79f0-4bbc-a99d-6adf735e2932"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'It\\'s a little disconcerting to have a character named Gig Young in a movie...played by Gig Young. But this film is where Gig got his name and also a nice career boost after playing small parts under another name.<br /><br />I\\'m going to go against the majority of the other comments and state that I really enjoyed this film, mainly because of the vibrant performance of Barbara Stanwyck as Fiona. She was funny, angry, vulnerable, caring, and feisty as the oldest of three daughters whose mother died on the Lusitania, and whose father was later killed during Woar War I. <br /><br />As the \"man\" of the house, Fiona has stood steadfast for years against settling her father\\'s will which would therefore allow a Donald Trump type named Charles Barclay to get the family home. But Fiona\\'s keeping a secret as to why she hates Barclay so much. Geraldine Fitzgerald is the middle, flirty sister, who is married to an Englishman but craves her youngest sister\\'s boyfriend (Gig Young).<br /><br />If you\\'re a Stanwyck fan, this is a no miss.'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOGTECCkx6AS",
        "outputId": "fd3a1301-9550-494f-de67-89caa51cb3d0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def review_to_words(review):\n",
        "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
        "    words = text.split() # Split string into words\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
        "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
        "    \n",
        "    return words"
      ],
      "metadata": {
        "id": "Y1nFQalsx_aW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
        "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
        "\n",
        "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
        "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
        "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
        "\n",
        "    # If cache_file is not None, try to read from it first\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = pickle.load(f)\n",
        "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass  # unable to read from cache, but that's okay\n",
        "    \n",
        "    # If cache is missing, then do the heavy lifting\n",
        "    if cache_data is None:\n",
        "        # Preprocess training and test data to obtain words for each review\n",
        "        #words_train = list(map(review_to_words, data_train))\n",
        "        #words_test = list(map(review_to_words, data_test))\n",
        "        words_train = [review_to_words(review) for review in data_train]\n",
        "        words_test = [review_to_words(review) for review in data_test]\n",
        "        \n",
        "        # Write to cache file for future runs\n",
        "        if cache_file is not None:\n",
        "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
        "                              labels_train=labels_train, labels_test=labels_test)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
        "    else:\n",
        "        # Unpack data loaded from cache file\n",
        "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
        "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
        "    \n",
        "    return words_train, words_test, labels_train, labels_test"
      ],
      "metadata": {
        "id": "u0nsD90YyDrK"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FudnwQxOyjFP",
        "outputId": "a633b3d7-8b0a-4a65-c55d-a509e7b5d390"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read preprocessed data from cache file: preprocessed_data.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Extract Bag-of-Words features**"
      ],
      "metadata": {
        "id": "-ptpLAj4yvFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import joblib\n",
        "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
        "\n",
        "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
        "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
        "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
        "    \n",
        "    # If cache_file is not None, try to read from it first\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = joblib.load(f)\n",
        "            print(\"Read features from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass  # unable to read from cache, but that's okay\n",
        "    \n",
        "    # If cache is missing, then do the heavy lifting\n",
        "    if cache_data is None:\n",
        "        # Fit a vectorizer to training documents and use it to transform them\n",
        "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
        "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
        "        vectorizer = CountVectorizer(max_features=vocabulary_size,\n",
        "                preprocessor=lambda x: x, tokenizer=lambda x: x)  # already preprocessed\n",
        "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
        "\n",
        "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
        "        features_test = vectorizer.transform(words_test).toarray()\n",
        "        \n",
        "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
        "        \n",
        "        # Write to cache file for future runs (store vocabulary as well)\n",
        "        if cache_file is not None:\n",
        "            vocabulary = vectorizer.vocabulary_\n",
        "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
        "                             vocabulary=vocabulary)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                joblib.dump(cache_data, f)\n",
        "            print(\"Wrote features to cache file:\", cache_file)\n",
        "    else:\n",
        "        # Unpack data loaded from cache file\n",
        "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
        "                cache_data['features_test'], cache_data['vocabulary'])\n",
        "    \n",
        "    # Return both the extracted features as well as the vocabulary\n",
        "    return features_train, features_test, vocabulary"
      ],
      "metadata": {
        "id": "k0Zim-pYyn4a"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, vocabulary = extract_BoW_features(train_X, test_X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad3dsO900drE",
        "outputId": "70fb0305-e20d-4913-8a8f-b27f9f5cbccd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read features from cache file: bow_features.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**classification **"
      ],
      "metadata": {
        "id": "A_LPSVCU6Idj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TODO: Split the train_X and train_y arrays into the DataFrames val_X, train_X and val_y, train_y. \n",
        "# Make sure that val_X and val_y contain 10 000 entires while train_X and train_y contain the remaining 15 000 entries.\n",
        "\n",
        "\n",
        "# Solution:\n",
        "# Earlier we shuffled the training dataset so to make things simple we can just assign\n",
        "# the first 10 000 reviews to the validation set and use the remaining reviews for training.\n",
        "val_X = pd.DataFrame(train_X[:10000])\n",
        "train_X = pd.DataFrame(train_X[10000:])\n",
        "\n",
        "val_y = pd.DataFrame(train_y[:10000])\n",
        "train_y = pd.DataFrame(train_y[10000:])"
      ],
      "metadata": {
        "id": "iOJnXcIe6AaS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '../data/xgboost'\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)"
      ],
      "metadata": {
        "id": "FFOp1y7n6YN8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
        "\n",
        "# TODO: Save the training and validation data to train.csv and validation.csv in the data_dir directory.\n",
        "#       Make sure that the files you create are in the correct format.\n",
        "\n",
        "# Solution:\n",
        "pd.concat([val_y, val_X], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
        "pd.concat([train_y, train_X], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
      ],
      "metadata": {
        "id": "7HepMUpK6d5b"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_X = train_X = val_X = train_y = val_y = None"
      ],
      "metadata": {
        "id": "nrii6Vge6q6-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "0mV23pxa6xKl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# fit model no training data\n",
        "model = XGBClassifier(max_depth=5,\n",
        "                        eta=0.2,\n",
        "                        gamma=4,\n",
        "                        min_child_weight=6,\n",
        "                        subsample=0.8,\n",
        "                        silent=0,\n",
        "                        objective='binary:logistic',\n",
        "                        early_stopping_rounds=10,\n",
        "                        num_round=500)"
      ],
      "metadata": {
        "id": "LkMiBzIl8Mbg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_X,train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0beu3oge8_tP",
        "outputId": "7c08635a-df96-431d-dbd0-ac234a8cbcd0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(early_stopping_rounds=10, eta=0.2, gamma=4, max_depth=5,\n",
              "              min_child_weight=6, num_round=500, silent=0, subsample=0.8)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(val_X)\n",
        "pd.DataFrame(preds).to_csv(os.path.join(data_dir, 'test_preds.csv'), header=False, index=False)"
      ],
      "metadata": {
        "id": "IIAHFUJ3A5CQ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(val_y, preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlzoraPHCBQX",
        "outputId": "6b60683f-c83e-44c8-838a-4da170c6fac0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8201"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mn=preds.min()\n",
        "mx=preds.max()\n",
        "mn\n",
        "mx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYhPdKOgFiIj",
        "outputId": "fd533970-d38b-4d29-9a60-11c74665c0ed"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First we will remove all of the files contained in the data_dir directory\n",
        "!rm $data_dir/*\n",
        "\n",
        "# And then we delete the directory itself\n",
        "!rmdir $data_dir\n",
        "\n",
        "# Similarly we will remove the files in the cache_dir directory and the directory itself\n",
        "!rm $cache_dir/*\n",
        "!rmdir $cache_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbB7oFzhKCeL",
        "outputId": "616aba75-273e-45cf-9594-20bf1a2d20c9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '../data/xgboost/*': No such file or directory\n",
            "rmdir: failed to remove '../data/xgboost': No such file or directory\n",
            "rm: cannot remove '../cache/sentiment_analysis/*': No such file or directory\n",
            "rmdir: failed to remove '../cache/sentiment_analysis': No such file or directory\n"
          ]
        }
      ]
    }
  ]
}